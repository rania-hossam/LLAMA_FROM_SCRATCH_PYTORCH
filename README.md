# LLAMA_FROM_SCRATCH_PYTORCH
[Read MY  Article on Medium](https://medium.com/@raniahossam/llama-from-scratch-00c1844ed4d9)

![LLAMA Image](https://github.com/rania-hossam/LLAMA_FROM_SCRATCH_PYTORCH/raw/master/images/photo_6048509869687946819_x.jpg)
Llama 2 is an updated collection of pre-trained and fine-tuned large language models (LLMs) introduced by Meta researchers. It encompasses models ranging from 7 billion to 70 billion parameters, each designed to deliver exceptional performance across various language processing tasks.
Building upon its predecssor, LLaMA, LLaMA 2 brings several enhancements. The pretraining corpus size has been expanded by 40%, allowing the model to learn from a more extensive and diverse set of publicly available data. Additionally, the context length of Llama 2 has been doubled, enabling the model to consider a more extensive context when generating responses, leading to improved output quality and accuracy


