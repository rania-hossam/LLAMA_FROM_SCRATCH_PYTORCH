# LLAMA_FROM_SCRATCH_PYTORCH
[Read MY  Article on Medium](https://medium.com/@raniahossam/llama-from-scratch-00c1844ed4d9)

![LLAMA Image](https://github.com/rania-hossam/LLAMA_FROM_SCRATCH_PYTORCH/raw/master/images/photo_6048509869687946819_x.jpg)

![LlaMA]([image1.jpg](https://github.com/rania-hossam/LLAMA_FROM_SCRATCH_PYTORCH/blob/master/images/photo_6048509869687946820_x.jpg))

## Introduction

 This document provides an overview of LlaMA, its different versions, and its capabilities and LlaMA (Large Language Model Meta AI) is a generative AI model.

## What is LlaMA?

LlaMA is a group of foundational Large Language Models developed by Meta AI. It was announced by Meta in February of 2023. LlaMA is available in various sizes, ranging from 7 billion to 65 billion parameters, with a context length of 2,000 tokens. These models are designed to assist researchers in advancing their knowledge in the field of AI. The smaller 7 billion parameter models enable researchers with limited computational resources to study and experiment with these models.

### Key Features

- LlaMA models come in multiple sizes(7B, 13B, and 70B), making them suitable for various applications
- Completely open-source and free for non-commercial use.

## After Update Steps: LlaMA 2

LlaMA 2 is the second version of LlaMA, surpassing the initial release, LlaMA version 1, which Meta introduced in July of 2023. LlaMA 2 is available in three sizes: 7 billion, 13 billion, and 70 billion parameter models. Upon its release, LlaMA 2 achieved the highest score on Hugging Face, a popular platform for AI model deployment and fine-tuning. Across all segments (7B, 13B, and 70B), LlaMA 2 models have consistently outperformed other models available on Hugging Face, thanks to fine-tuning and retraining efforts.



